# -*- coding: utf-8 -*-
"""word 2 vec cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T1CEanlKoVd-juU84vfivuK8ncTZa4sM
"""

pip install transformers

pip install --upgrade transformers

text_dataset = "/content/IMDB Dataset.csv"

# Define DataLoader objects for train and test sets
batch_size = 32  # Adjust as needed
train_size = int(0.8 * len(text_dataset))
test_size = len(text_dataset) - train_size
train_dataset, test_dataset = random_split(text_dataset, [train_size, test_size])
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

import numpy as np
import pandas as pd
from gensim.models.word2vec import Word2Vec
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import PorterStemmer
from sklearn import preprocessing

def preprocess_text(sen):
    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sen)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    stops = stopwords.words('english')
    #print(stops)
    porter = PorterStemmer()
    for word in sentence.split():
        if word in stops:
            sentence = sentence.replace(word, '')
        sentence = sentence.replace(word, porter.stem(word))
    return sentence.lower()

pip install nltk

import nltk
nltk.download('stopwords')

mes = []
for i in df['review']:
    mes.append(i.split())
print(mes[:2])

# Assuming you have the user input stored in a variable called user_input
user_input = "Your user input goes here"

# Preprocess user input
# Perform tokenization, convert words to indices based on the vocabulary used during training
# Pad or truncate the input to match the input size expected by the model

# Make predictions
predicted_probabilities = keras_model.predict(user_input)

# Assuming the model outputs probabilities for two classes (binary classification)
# You can use argmax to get the predicted class
predicted_class = np.argmax(predicted_probabilities)

# Print the predicted class
print("Predicted Class:", predicted_class)

# If you have the actual class for comparison (ground truth), you can calculate accuracy
# Assuming actual_class is the ground truth class (0 or 1)
# Calculate accuracy
# If predicted class matches the actual class, it's a correct prediction
# Otherwise, it's incorrect
accuracy = 1 if predicted_class == actual_class else 0
print("Accuracy:", accuracy)

pip install gensim nltk

word2vec_model = Word2Vec(mes, window=3, min_count=1, workers=16)
print(word2vec_model)

token = Tokenizer(7229)
token.fit_on_texts(df['review'])
text = token.texts_to_sequences(df['review'])
text = pad_sequences(text, 75)
print(text[:2])

le = preprocessing.LabelEncoder()
y = le.fit_transform(df['sentiment'])
y = to_categorical(y)
y[:2]

df = pd.read_csv('/content/tweet.csv')
df['Message'] = df['Message'].apply(preprocess_text)
df.head(10)

mes = []
for i in df['Message']:
    mes.append(i.split())
print(mes[:2])

word2vec_model = Word2Vec(mes, window=3, min_count=1, workers=16)
print(word2vec_model)

token = Tokenizer(7229)
token.fit_on_texts(df['Message'])
text = token.texts_to_sequences(df['Message'])
text = pad_sequences(text, 75)
print(text[:2])

le = preprocessing.LabelEncoder()
y = le.fit_transform(df['Category'])
y = to_categorical(y)
y[:2]

import numpy as np

x_train, x_test, y_train, y_test = train_test_split(np.array(text), y, test_size=0.2, stratify=y)

from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, Activation
import numpy as np

# Assuming you have already trained or loaded your Word2Vec model
word2vec_model = Word2Vec(sentences=mes, vector_size=100, window=3, min_count=1, workers=16)

# Create Keras model
keras_model = Sequential()
embedding_matrix = np.zeros((len(word2vec_model.wv.key_to_index) + 1, 100))  # Adjust the size accordingly

for word, i in word2vec_model.wv.key_to_index.items():
    embedding_matrix[i] = word2vec_model.wv[word]

embedding_layer = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],
                            weights=[embedding_matrix], trainable=False)
keras_model.add(embedding_layer)
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(50, 3, activation='relu', padding='same', strides=1))
keras_model.add(Conv1D(50, 3, activation='relu', padding='same', strides=1))
keras_model.add(MaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(100, 3, activation='relu', padding='same', strides=1))
keras_model.add(Conv1D(100, 3, activation='relu', padding='same', strides=1))
keras_model.add(MaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(200, 3, activation='relu', padding='same', strides=1))
keras_model.add(Conv1D(200, 3, activation='relu', padding='same', strides=1))
keras_model.add(GlobalMaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Dense(200))
keras_model.add(Activation('relu'))
keras_model.add(Dropout(0.2))
keras_model.add(Dense(2))
keras_model.add(Activation('softmax'))
keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')
keras_model.summary()

# Assuming you have x_train, y_train, x_test, y_test prepared
keras_model.fit(x_train, y_train, batch_size=16, epochs=10, validation_data=(x_test, y_test))

from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, Activation
import numpy as np

# Assuming you have already trained or loaded your Word2Vec model
word2vec_model = Word2Vec(sentences=mes, vector_size=100, window=3, min_count=1, workers=16)

# Create Keras model
keras_model = Sequential()
embedding_matrix = np.zeros((len(word2vec_model.wv.key_to_index) + 1, 100))  # Adjust the size accordingly

for word, i in word2vec_model.wv.key_to_index.items():
    embedding_matrix[i] = word2vec_model.wv[word]

embedding_layer = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],
                            weights=[embedding_matrix], trainable=False)
keras_model.add(embedding_layer)
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(50, 3, activation='swish', padding='same', strides=1))
keras_model.add(Conv1D(50, 3, activation='swish', padding='same', strides=1))
keras_model.add(MaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(100, 3, activation='swish', padding='same', strides=1))
keras_model.add(Conv1D(100, 3, activation='swish', padding='same', strides=1))
keras_model.add(MaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(200, 3, activation='swish', padding='same', strides=1))
keras_model.add(Conv1D(200, 3, activation='swish', padding='same', strides=1))
keras_model.add(GlobalMaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Dense(200))
keras_model.add(Activation('swish'))
keras_model.add(Dropout(0.2))
keras_model.add(Dense(2))
keras_model.add(Activation('softmax'))
keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')
keras_model.summary()

# Assuming you have x_train, y_train, x_test, y_test prepared
keras_model.fit(x_train, y_train, batch_size=16, epochs=10, validation_data=(x_test, y_test))

from sklearn.metrics import classification_report

# Assuming you have x_train, y_train, x_test, y_test prepared
history = keras_model.fit(x_train, y_train, batch_size=16, epochs=30, validation_data=(x_test, y_test))
# Evaluate the model on the test set
_, accuracy = keras_model.evaluate(x_test, y_test)
# Predict probabilities for test set
y_pred_probs = keras_model.predict(x_test)
# Convert probabilities to classes
y_pred = np.argmax(y_pred_probs, axis=1)
# Convert y_test from one-hot encoded to integer labels
y_test_labels = np.argmax(y_test, axis=1)
# Print accuracy
print("Accuracy:", accuracy)
# Print classification report
print(classification_report(y_test_labels, y_pred))

import numpy as np
word2vec_model = Word2Vec(sentences=mes, vector_size=100, window=3, min_count=1, workers=16)
def preprocess_text(text):
  # Replace with your specific preprocessing steps (e.g., tokenization, case conversion, stop word removal)
  preprocessed_text = text.lower().split()  # Example tokenization
  word_indexes = [word2vec_model.wv.key_to_index.get(word, 0) for word in preprocessed_text]
  max_sequence_length = 100


  if len(word_indexes) > max_sequence_length:
    word_indexes = word_indexes[:max_sequence_length]
  else:
    word_indexes += [0] * (max_sequence_length - len(word_indexes))

  return word_indexes, np.expand_dims(word_indexes, axis=0)
# User input
user_input = "An awful film! It must have been up against some real stinkers to be nominated for the Golden Globe. They've taken the story of the first famous female Renaissance painter and mangled it beyond recognition. "

# Preprocess the user input
word_indexes, user_input_vector = preprocess_text(user_input)
predicted_probabilities = keras_model.predict(user_input_vector)
predicted_class = np.argmax(predicted_probabilities)

# Print the predicted class
print("Predicted Class:", predicted_class)

actual_class = 1
accuracy = 1 if predicted_class == actual_class else 0
print("Accuracy:", accuracy)
import seaborn as sns
sns.set_style('whitegrid')
sns.countplot(x='label',data=df, palette='YlGnBu_r')

import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Define preprocessing function
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Lowercasing
    tokens = [word.lower() for word in tokens]

    # Stopword removal
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if not word in stop_words]

    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]

    # Removing special characters and punctuation
    tokens = [word for word in tokens if word.isalnum()]

    # Joining the tokens back into a string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

# Apply preprocessing function to dataset
df['preprocessed_text'] = df['review'].apply(preprocess_text)

df['preprocessed_text'].head(10)

df.isna().sum()

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['preprocessed_text'], df['label'], test_size=0.1, random_state=42)

X_train.shape

X_test.shape

# Extract bag-of-words features from the training and testing sets
vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

# Transform bag-of-words features into TF-IDF features
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
X_test_tfidf = tfidf_transformer.transform(X_test_counts)

"""Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train_tfidf, y_train)

# Predict the labels of the testing data and convert the predictions to binary labels
y_pred = nb.predict(X_test_tfidf)
y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]

# Predict the labels of the testing data and convert the predictions to binary labels
y_pred = nb.predict(X_test_tfidf)
y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]

from sklearn.metrics import ConfusionMatrixDisplay,classification_report
print(classification_report(y_test,y_pred))

"""Logistic regression"""

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(max_iter=1000)
log.fit(X_train_tfidf, y_train)

from sklearn.metrics import ConfusionMatrixDisplay,classification_report
def report(model):
    preds = model.predict(X_test_tfidf)
    preds = pd.Series(preds).apply(lambda x: 0 if x < 0.5 else (1 if x < 1.5 else 2))
    print(classification_report(y_test,preds))
report(log)

"""SVC"""

from sklearn.svm import LinearSVC
svc = LinearSVC()
svc.fit(X_train_tfidf, y_train)

report(svc)

"""Random forest"""

from sklearn.ensemble import RandomForestClassifier

clf_rf = RandomForestClassifier()
clf_rf.fit(X_train_tfidf, y_train)
y_pred_rf = clf_rf.predict(X_test_tfidf)
y_pred_rf = [1 if pred > 0.5 else 0 for pred in y_pred_rf]
report(clf_rf)

"""Decision tree"""

from sklearn.tree import DecisionTreeClassifier
clf_dt = DecisionTreeClassifier()
clf_dt.fit(X_train_tfidf, y_train)
y_pred_dt = clf_dt.predict(X_test_tfidf)
y_pred_dt = [1 if pred > 0.5 else 0 for pred in y_pred_dt]

report(clf_dt)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')