# -*- coding: utf-8 -*-
"""word 2 vec cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T1CEanlKoVd-juU84vfivuK8ncTZa4sM
"""

pip install transformers

pip install --upgrade transformers

text_dataset = "/content/IMDB Dataset.csv"

# Define DataLoader objects for train and test sets
batch_size = 32  # Adjust as needed
train_size = int(0.8 * len(text_dataset))
test_size = len(text_dataset) - train_size
train_dataset, test_dataset = random_split(text_dataset, [train_size, test_size])
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

import numpy as np
import pandas as pd
from gensim.models.word2vec import Word2Vec
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import PorterStemmer
from sklearn import preprocessing

def preprocess_text(sen):
    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sen)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    stops = stopwords.words('english')
    #print(stops)
    porter = PorterStemmer()
    for word in sentence.split():
        if word in stops:
            sentence = sentence.replace(word, '')
        sentence = sentence.replace(word, porter.stem(word))
    return sentence.lower()

pip install nltk

import nltk
nltk.download('stopwords')

mes = []
for i in df['review']:
    mes.append(i.split())
print(mes[:2])

# Assuming you have the user input stored in a variable called user_input
user_input = "Your user input goes here"

# Preprocess user input
# Perform tokenization, convert words to indices based on the vocabulary used during training
# Pad or truncate the input to match the input size expected by the model

# Make predictions
predicted_probabilities = keras_model.predict(user_input)

# Assuming the model outputs probabilities for two classes (binary classification)
# You can use argmax to get the predicted class
predicted_class = np.argmax(predicted_probabilities)

# Print the predicted class
print("Predicted Class:", predicted_class)

# If you have the actual class for comparison (ground truth), you can calculate accuracy
# Assuming actual_class is the ground truth class (0 or 1)
# Calculate accuracy
# If predicted class matches the actual class, it's a correct prediction
# Otherwise, it's incorrect
accuracy = 1 if predicted_class == actual_class else 0
print("Accuracy:", accuracy)

pip install gensim nltk

word2vec_model = Word2Vec(mes, window=3, min_count=1, workers=16)
print(word2vec_model)

token = Tokenizer(7229)
token.fit_on_texts(df['review'])
text = token.texts_to_sequences(df['review'])
text = pad_sequences(text, 75)
print(text[:2])

le = preprocessing.LabelEncoder()
y = le.fit_transform(df['sentiment'])
y = to_categorical(y)
y[:2]

df = pd.read_csv('/content/tweet.csv')
df['Message'] = df['Message'].apply(preprocess_text)
df.head(10)

mes = []
for i in df['Message']:
    mes.append(i.split())
print(mes[:2])

word2vec_model = Word2Vec(mes, window=3, min_count=1, workers=16)
print(word2vec_model)

token = Tokenizer(7229)
token.fit_on_texts(df['Message'])
text = token.texts_to_sequences(df['Message'])
text = pad_sequences(text, 75)
print(text[:2])

le = preprocessing.LabelEncoder()
y = le.fit_transform(df['Category'])
y = to_categorical(y)
y[:2]

import numpy as np

x_train, x_test, y_train, y_test = train_test_split(np.array(text), y, test_size=0.2, stratify=y)

from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, Activation
import numpy as np

# Assuming you have already trained or loaded your Word2Vec model
word2vec_model = Word2Vec(sentences=mes, vector_size=100, window=3, min_count=1, workers=16)

# Create Keras model
keras_model = Sequential()
embedding_matrix = np.zeros((len(word2vec_model.wv.key_to_index) + 1, 100))  # Adjust the size accordingly

for word, i in word2vec_model.wv.key_to_index.items():
    embedding_matrix[i] = word2vec_model.wv[word]

embedding_layer = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],
                            weights=[embedding_matrix], trainable=False)
keras_model.add(embedding_layer)
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(50, 3, activation='relu', padding='same', strides=1))
keras_model.add(Conv1D(50, 3, activation='relu', padding='same', strides=1))
keras_model.add(MaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(100, 3, activation='relu', padding='same', strides=1))
keras_model.add(Conv1D(100, 3, activation='relu', padding='same', strides=1))
keras_model.add(MaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(200, 3, activation='relu', padding='same', strides=1))
keras_model.add(Conv1D(200, 3, activation='relu', padding='same', strides=1))
keras_model.add(GlobalMaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Dense(200))
keras_model.add(Activation('relu'))
keras_model.add(Dropout(0.2))
keras_model.add(Dense(2))
keras_model.add(Activation('softmax'))
keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')
keras_model.summary()

# Assuming you have x_train, y_train, x_test, y_test prepared
keras_model.fit(x_train, y_train, batch_size=16, epochs=10, validation_data=(x_test, y_test))

from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, Activation
import numpy as np

# Assuming you have already trained or loaded your Word2Vec model
word2vec_model = Word2Vec(sentences=mes, vector_size=100, window=3, min_count=1, workers=16)

# Create Keras model
keras_model = Sequential()
embedding_matrix = np.zeros((len(word2vec_model.wv.key_to_index) + 1, 100))  # Adjust the size accordingly

for word, i in word2vec_model.wv.key_to_index.items():
    embedding_matrix[i] = word2vec_model.wv[word]

embedding_layer = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],
                            weights=[embedding_matrix], trainable=False)
keras_model.add(embedding_layer)
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(50, 3, activation='swish', padding='same', strides=1))
keras_model.add(Conv1D(50, 3, activation='swish', padding='same', strides=1))
keras_model.add(MaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(100, 3, activation='swish', padding='same', strides=1))
keras_model.add(Conv1D(100, 3, activation='swish', padding='same', strides=1))
keras_model.add(MaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Conv1D(200, 3, activation='swish', padding='same', strides=1))
keras_model.add(Conv1D(200, 3, activation='swish', padding='same', strides=1))
keras_model.add(GlobalMaxPooling1D())
keras_model.add(Dropout(0.2))
keras_model.add(Dense(200))
keras_model.add(Activation('swish'))
keras_model.add(Dropout(0.2))
keras_model.add(Dense(2))
keras_model.add(Activation('softmax'))
keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')
keras_model.summary()

# Assuming you have x_train, y_train, x_test, y_test prepared
keras_model.fit(x_train, y_train, batch_size=16, epochs=10, validation_data=(x_test, y_test))

from sklearn.metrics import classification_report

# Assuming you have x_train, y_train, x_test, y_test prepared
history = keras_model.fit(x_train, y_train, batch_size=16, epochs=30, validation_data=(x_test, y_test))
# Evaluate the model on the test set
_, accuracy = keras_model.evaluate(x_test, y_test)
# Predict probabilities for test set
y_pred_probs = keras_model.predict(x_test)
# Convert probabilities to classes
y_pred = np.argmax(y_pred_probs, axis=1)
# Convert y_test from one-hot encoded to integer labels
y_test_labels = np.argmax(y_test, axis=1)
# Print accuracy
print("Accuracy:", accuracy)
# Print classification report
print(classification_report(y_test_labels, y_pred))

import numpy as np
word2vec_model = Word2Vec(sentences=mes, vector_size=100, window=3, min_count=1, workers=16)
def preprocess_text(text):
  # Replace with your specific preprocessing steps (e.g., tokenization, case conversion, stop word removal)
  preprocessed_text = text.lower().split()  # Example tokenization
  word_indexes = [word2vec_model.wv.key_to_index.get(word, 0) for word in preprocessed_text]
  max_sequence_length = 100


  if len(word_indexes) > max_sequence_length:
    word_indexes = word_indexes[:max_sequence_length]
  else:
    word_indexes += [0] * (max_sequence_length - len(word_indexes))

  return word_indexes, np.expand_dims(word_indexes, axis=0)
# User input
user_input = "An awful film! It must have been up against some real stinkers to be nominated for the Golden Globe. They've taken the story of the first famous female Renaissance painter and mangled it beyond recognition. "

# Preprocess the user input
word_indexes, user_input_vector = preprocess_text(user_input)
predicted_probabilities = keras_model.predict(user_input_vector)
predicted_class = np.argmax(predicted_probabilities)

# Print the predicted class
print("Predicted Class:", predicted_class)

actual_class = 1
accuracy = 1 if predicted_class == actual_class else 0
print("Accuracy:", accuracy)