# -*- coding: utf-8 -*-
"""IMDB DATASET.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ul9X_REyaJeJ-nlMxqWPCZgzJc8iWRNY

IMDB dataset -review # New Section
"""

import numpy as np
import pandas as pd
import nltk

nltk.download('stopwords')

nltk.download('punkt')

import csv

pip install pandas

df = pd.read_csv('/content/IMDB Dataset.csv')

df.head(10)

df.columns

df.shape

df = df.sample(n=30000) # Select 30000 random rows of your dataset

df.shape

df.columns

df['label'] = df['sentiment'].map({'positive':1,'negative':0})

import seaborn as sns
sns.set_style('whitegrid')
sns.countplot(x='label',data=df, palette='YlGnBu_r')

import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Define preprocessing function
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Lowercasing
    tokens = [word.lower() for word in tokens]

    # Stopword removal
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if not word in stop_words]

    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]

    # Removing special characters and punctuation
    tokens = [word for word in tokens if word.isalnum()]

    # Joining the tokens back into a string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

# Apply preprocessing function to dataset
df['preprocessed_text'] = df['review'].apply(preprocess_text)

df['preprocessed_text'].head(10)

df.isna().sum()

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['preprocessed_text'], df['label'], test_size=0.1, random_state=42)

X_train.shape

X_test.shape

# Extract bag-of-words features from the training and testing sets
vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

# Transform bag-of-words features into TF-IDF features
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
X_test_tfidf = tfidf_transformer.transform(X_test_counts)

"""Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train_tfidf, y_train)

# Predict the labels of the testing data and convert the predictions to binary labels
y_pred = nb.predict(X_test_tfidf)
y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]

# Predict the labels of the testing data and convert the predictions to binary labels
y_pred = nb.predict(X_test_tfidf)
y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]

from sklearn.metrics import ConfusionMatrixDisplay,classification_report
print(classification_report(y_test,y_pred))

"""Logistic regression"""

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(max_iter=1000)
log.fit(X_train_tfidf, y_train)

from sklearn.metrics import ConfusionMatrixDisplay,classification_report
def report(model):
    preds = model.predict(X_test_tfidf)
    preds = pd.Series(preds).apply(lambda x: 0 if x < 0.5 else (1 if x < 1.5 else 2))
    print(classification_report(y_test,preds))
report(log)

"""SVC"""

from sklearn.svm import LinearSVC
svc = LinearSVC()
svc.fit(X_train_tfidf, y_train)

report(svc)

"""Random forest"""

from sklearn.ensemble import RandomForestClassifier

clf_rf = RandomForestClassifier()
clf_rf.fit(X_train_tfidf, y_train)
y_pred_rf = clf_rf.predict(X_test_tfidf)
y_pred_rf = [1 if pred > 0.5 else 0 for pred in y_pred_rf]
report(clf_rf)

"""Decision tree"""

from sklearn.tree import DecisionTreeClassifier
clf_dt = DecisionTreeClassifier()
clf_dt.fit(X_train_tfidf, y_train)
y_pred_dt = clf_dt.predict(X_test_tfidf)
y_pred_dt = [1 if pred > 0.5 else 0 for pred in y_pred_dt]

report(clf_dt)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

"""CATBOOST"""

pip install catboost

import catboost
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score



# Initialize the CatBoostClassifier
model = CatBoostClassifier()

# Train the model
s=model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_tfidf)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

report(model)

"""XGboost"""

pip install xgboost

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Initialize the XGBoost classifier
xgmodel = xgb.XGBClassifier()

# Train the model
xgmodel.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = xgmodel.predict(X_test_tfidf)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

report(xgmodel)

"""STOCHASTIC GRADIENT DESCENT"""

from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Initialize the SGDClassifier
sgdmodel = SGDClassifier(loss='log', max_iter=1000, random_state=42)

# Train the model
sgdmodel.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = sgdmodel.predict(X_test_tfidf)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

report(sgdmodel)

from sklearn.svm import LinearSVC
svc = LinearSVC()
svc.fit(X_train_tfidf, y_train)

report(svc)

# Train an SVR model with a linear kernel on the training data
svr = SVR(kernel='linear')
svr.fit(X_train_tfidf, y_train)

import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

# x-coordinates of left sides of bars
left = [1, 2, 3, 4, 5,6,7]

# heights of bars
height = [70,85,89,85,71,89,98]

# labels for bars
tick_label = ['SVR', 'NB', 'LR', 'RF', 'DT','SVC','CNN']

# plotting a bar chart
plt.bar(left, height, tick_label = tick_label,
		width = 0.4, color = ['blue'])

# naming the x-axis
plt.xlabel('different model')
# naming the y-axis
plt.ylabel('Accuracy value')
# plot title
plt.title('Comparison between different models and its accuracy values')

# function to show the plot
plt.show()

import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

# x-coordinates of left sides of bars
left = [1, 2, 3]

# heights of bars
height = [85,86,88]

# labels for bars
tick_label = ['XGBOOST','CATBOOST','SGD']

# plotting a bar chart
plt.bar(left, height, tick_label = tick_label,
		width = 0.3, color = ['blue', 'pink'])

# naming the x-axis
plt.xlabel('BOOSTING TECHNIQUES')
# naming the y-axis
plt.ylabel('Accuracy value')
# plot title
plt.title('Comparison between Boosting techniques and accuracy values')

# function to show the plot
plt.show()